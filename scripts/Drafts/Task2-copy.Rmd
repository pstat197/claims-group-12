---
title: "Task2-Copy"
author: "Brian Che"
date: "2022-11-11"
output: html_document
---

Perform a secondary tokenization of the data to obtain bigrams. Fit a logistic principal component regression model to the word-tokenized data, and then input the predicted log-odds-ratios together with some number of principal components of the bigram-tokenized data to a second logistic regression model. Based on the results, does it seem like the bigrams capture additional information about the claims status of a page?

```{r}
# setup
library(tidyverse)
library(tidytext)
library(tokenizers)
library(textstem)
library(stopwords)
```

```{r}
#load('/Users/brian/Documents/GitHub/claims-group-12/data/claims-raw.RData')
# preprocess (will take a minute or two)
#claims_clean <- claims_raw %>%
  #parse_data()
#claims_clean
```
```{r}
load('/Users/brian/Documents/GitHub/claims-group-12/data/claims-raw.RData')
load('/Users/brian/Documents/GitHub/claims-group-12/data/claims-test.Rdata')
load('/Users/brian/Documents/GitHub/claims-group-12/data/claims-clean-example.Rdata')
```
```{r}
ls()
```

```{r}
# function to parse html and clean text
parse_fn <- function(.html){
  read_html(.html) %>%
    html_elements('p') %>%
    html_text2() %>%
    str_c(collapse = ' ') %>%
    rm_url() %>%
    rm_email() %>%
    str_remove_all('\'') %>%
    str_replace_all(paste(c('\n', 
                            '[[:punct:]]', 
                            'nbsp', 
                            '[[:digit:]]', 
                            '[[:symbol:]]'),
                          collapse = '|'), ' ') %>%
    str_replace_all("([a-z])([A-Z])", "\\1 \\2") %>%
    tolower() %>%
    str_replace_all("\\s+", " ")
}

# function to apply to claims data
parse_data <- function(.df){
  out <- .df %>%
    filter(str_detect(text_tmp, '<!')) %>%
    rowwise() %>%
    mutate(text_clean = parse_fn(text_tmp)) %>%
    unnest(text_clean) 
  return(out)
}

nlp_fn2 <- function(parse_data.out){
  out <- parse_data.out %>% 
    unnest_tokens(output = token, 
                  input = text_clean, 
                  token = 'ngrams',
                  n = 2,
                  stopwords = str_remove_all(stop_words$word, 
                                             '[[:punct:]]')) %>%
    mutate(token.lem = lemmatize_words(token)) %>%
    filter(str_length(token.lem) > 2) %>%
    count(.id, bclass, token.lem, name = 'n') %>%
    bind_tf_idf(term = token.lem, 
                document = .id,
                n = n) %>%
    pivot_wider(id_cols = c('.id', 'bclass'),
                names_from = 'token.lem',
                values_from = 'tf_idf',
                values_fill = 0)
  return(out)
}

```


```{r}
# Task Two
claims_bigrams <- claims_clean %>% 
  nlp_fn2()
```

```{r}
# partition data
set.seed(198888)
partitions <- claims_bigrams %>% initial_split(prop = 0.8)
```


```{r}
cc=claims_clean[,c('Age','Gender')]
claims_clean %>% tokenize_words()
```


```{r}
```


```{r}
invitation_text %>% claims_clean(n = 2)
```


```{r}
```


```{r}
stpwrd <- stop_words %>%
  pull(word) %>%
  str_remove_all('[[:punct:]]')

claims_tokens_long <- claims_clean %>%
  unnest_tokens(output = token, # specifies new column name
                input = text, # specifies column containing text
                token = 'words', # how to tokenize
                stopwords = stpwrd) %>% # optional stopword removal
  mutate(token = lemmatize_words(token)) 
```
```{r}
install.packages("magrittr") # package installations are only needed the first time you use it
install.packages("dplyr")    # alternative installation of the %>%
library(magrittr) # needs to be run every time you start R and want to use %>%
library(dplyr)    # alternatively, this also loads %>%
```

```{r}
library(tokenizers)
#invitation_text %>% tokenize_words()
invitation_text %>% tokenize_ngrams(n = 2)
```

